**Jay Gordon:** you're listening to the On-Call nightmares podcasts, and every week I bring you conversations with technologists who spent time on call. This week, I speak to Jason Hand of Microsoft, a coworker of mine and a really great person that I appreciate, you know, working in this business, get some opportunities to meet really incredible people, and sometimes you get to work with them. And Jason's one of them. Jason's a great guy. Who I really feel is part of helping people make better decisions about technology, helping people learn more about what it is they can do in the cloud. And, um, I'm really thankful that he gave me a few minutes while he was here in New York City. So one cool thing is, I got some new intro music. I'm gonna try this for a little bit because I'm into trap beats. What's up, everybody? And also I was it wasn't a vacation last week, got spent some time in Hawaii and let me tell you what a beautiful place, really thankful that I got to do that trip and sorry, I didn't have a chauffeur you last week, but it is what it is. Why I kind of tow precedence, if you will. So let's get into the conversation with Jason. But before we get into that discussion, don't forget we always want you to be part of this. So if you like to be on this podcast, the easiest way is just sent me a notice. It's @jaydestro on Twitter. It's all @oncallnightmare on Twitter. Or you could just email at very easily oncallnightmares@gmail.com. Let's get into the conversation with Jason now 

**Jay Gordon:** You're listening to the On-Call Nightmares podcast and just about every week, except for what amount of town. And I'm on vacation, which was really cool. And I'll get into that at some other point. This podcast, I'm sure, but probably in the intro that I do before this interview. But regardless I'm going to bring you an interview with someone that is not only just a part of the team that I work on, but it's somewhat I really consider a friend. And that I'm really lucky is, uh, you know, been someone that I just met on the road, and, uh, it kind of went from there. So his name is Jason Hand. He is part of the startups team or what you call it it here in Microsoft's cloud advocacy team. What's up, Jason? How's a guy? I think the coolest part is that we're getting to do this face to face. And we're doing it here in my apartment because you were a Q-Conn this week, right?

Jason Hand: Yeah. I was just there yesterday and giving a talk. Um, yeah. So it's good to be here and hang out with you before I got to go with the airport.

**Jay Gordon:** Thank you. I appreciate it. So one of the big kind of conversations that this podcast is bit about is, you know, on call and how people have made on call suck less. And for a long time I really believe that that was one of your big jobs. You you talk to people around the world for a while about how to make a phone call, suck less. And now you're really in a position where you're you're looking at the grand scheme off technology and maybe making I t implementation suck less, Would you say

Jason Hand: Yeah, I guess that's what a good way to put it. And It's funny, you say, making on call suck less. Because prior to joining Microsoft, I was with Victor Ops and and that was their tagline. That's the slogan that they use way make on-call suck less. And it really speaks to a lot of people because anybody who's been on call knows whether, you know, regardless of what feelings you have toward that language in general, that being on call is not easy. It's it's it's, you know, cognitively draining. It's physically draining. It's it's just not something that's often unpleasant. And so a big part of what I was doing while I was at Victor Ops was trying to share ideas that make it just a little bit, you know, a little bit easier. So we don't have this like fear of being on call or this distaste of that whole responsibility, which more and more we're realizing it is important because we're building things that people do you rely on, and in some cases they rely on for their health, rely on them for their safety. They rely on them for their their lives, in general, their livelihood. And it's not just a matter of trying to keep our social media applications up running. It's actually trying to keep service's and tech. That is really important to people trying to keep that up and running. So it's become a much Maur important topic. I think as the world has become digitally connected,

**Jay Gordon:** well, liability is kind of the big part of, um, I guess the whole reliability of both word because we've been talking about reliability is, ah, I guess, an industry for a very long time. But I feel like the focus has been taken off less of performance, and it's more on reliability. Reliability began performance, I think, and that's where I think this whole industry's going. But before we kind of get a little bit further to that, I want to find out where you came from, and that's one of my favorite parts about this podcast is I get to bring it all the way, all the way back and ask you So how how did you get started on call?

Jason Hand: OK, well, if I sort of back all the way up to gosh right around just prior to your 2000 you're the very first big concern I had was the 2000

**Jay Gordon:** Sure, Y2K

Jason Hand: and that was right out. I ran out of college. I started working for manufacturing company, actually was sort of the family business that I was working for, and I stepped into the role of just sort of a head of I T and had a small team on was pretty much responsible for all of our internal servers. This was way before Cloud, we had our own sort of internal mainframe ARP system. We had our own well. Eventually we had our own internal email server. He's an exchange which later we moved to an open source solution. But it was, Yeah, it was. It was really in that environment where I know the buck stopped with me. And even though I had a team, I was the one who took on pretty much all responsibility, for I would always joke if it's plugged into a wall, then I'm probably responsible for it because it went from our mainframe, always our fax machines in the phone system. Really, if electricity was running through it, people would come to me when it wasn't working.

**Jay Gordon:** So it's that early, early idea of on call where there's someone in the house on. They smell a little bit of smoke. You're the first fireman they're going through. I'm guessing they're not even gonna look. If there's fire, they're just going right to the fireman. 

Jason Hand: Yes, exactly. And most, you know, they knew that I was probably the most technical person within the company, but also someone who, you know, was just always willing to help. Even if it didn't feel like I was part of my responsibility, I was able to come in and make things easier, so that's sort of my first, you know, Not that I was always being paged in that kind of thing, but people would come to me when there was a problem.

**Jay Gordon:** You were the person early on that helped that business, make sure that the technology that was put in place or needed to be put in place happened. And that's a kind of familiar, I think, role for a lot of people in the late nineties, early two thousands, a lot of practitioners, that's how we got our started, is that someone somewhere needed a person that was kind of capable of organizing all that stuff. In the end, there was a lot of organization before you even plugged anything into the wall. It was like, who? My vendors. Who am I gonna make relationships with who's gonna end up sending me the phone system? How much is the phone system support going to cost? So, yeah, those were days that you've learned so much about, I guess Internal I t Before you even had to worry about wet product.

Jason Hand: Totally. I mean, that's where I think I've developed what they call now that the T shape is that I could try to specialize in certain things, but because I was irresponsible and involved with so much, I had to sort of spread out my time and my my energy and becoming an expertise and all that stuff.

**Jay Gordon:** And then?

Jason Hand: Then from there, you know, about fast forward, maybe 10 years. Um, I decided to make a change, and I got into the start up world. Sure. And I took a role as the head of technical support for a company in Boulder, Colorado, called Standing Cloud, which was kind of a cloud broker where you could move, you could install applications on literally any cloud that you want, and then, with a few clicks, you could transfer to a different clouds you could start off on. Maybe eight of us realize. Yeah, it's, you know, it's it's reliable service. It's quick, but it's also super expensive. So let me try. You know, Brax Base, let me try software. Let me try forever. So it made your applications and your infrastructure very portable. And that really was, I think, where I sunk my teeth into not only the cloud but also the operations side of things where you're starting to build up, build up infrastructure, using code one of cases, and it's not. It's not a matter of me using a service like CDW and buying a bunch of servers that I think I'm gonna have to, you know, Rackham and get him all networks and all that stuff. You don't have to do any of that. I would do that from a keyboard. Sure, that's the world that we're kind of in now, or at least a lot of us are moving to where it's not. It isn't these physical servers that we're dealing with.

**Jay Gordon:** Yes, and I think a lot of us in our lower back are thankful for. So you move into this. And in this rule, have Are you part of a formal rotation, or is this still kind of an ad hoc? Well, it

Jason Hand: was a small team. There's only about 20 of us. So I was part of even though I was head of support. A lot of times I would receive, of course, the first, you know, notice of a problem with that a customer was having. And then I would deal with our work with our operations teams, kind of in a coordinated effort to figure out what's going on. We'll make some improvements or remediate whatever issues were going on. So I you know, I was a pager duty user, and I I had to go through. You know, we have rotations, and we were. Then it seemed like a good idea to push out our big feature releases on Friday evenings afternoons, which now we know it is a horrible idea. But yeah, that was another place where I really started to learn some of the nuance of being on call and having a coordinated response to and having a team of people that you know you know, maybe I'm the first responder and I'm the first person who kind of digs in, But there's gonna come a time where, as I'm looking through whatever information I have, I realize this is this is beyond my level. Or maybe I don't have access into a system where I need to go do some more digging. So I need to escalate, is what we would say and I need to go find somebody who does have access to the system. Who does know a little bit, Maur. And that's where I started seeing how you know you can really come up with a much more I always just a coordinated response rather than being this reactionary kind of knee jerk behavior. Whenever something goes wrong, it's more of a mind shift. You know something's gonna go wrong eventually,

**Jay Gordon:** and it sounds like it's like a foreshadowing for where your career starts to move. Oh,

Jason Hand: absolutely, it is. Yeah, I spent time at standing clowned, and when that company was acquired shortly thereafter, I then moved over to Victor Op, see where the head of product was also on employees over standing clouds had that relationship and she knew that. You know, I had become very, I guess, familiar with the on call responsibility and and how to make that better. And so when they were looking for someone to be their technology of Angelus and sort of get out there and talk about really Dev ops principles and some of the stuff she came to me and I was super excited to come join Victor ops and do what I could sort of improve everyone's scenario with on call.

**Jay Gordon:** And like many of us who go into advocacy and evangelism, it's when you finally get off the on Paul rotation and you start learning about how to explain to people what it's late to stop being wrong. Cole. Yeah, and that's a whole other kind of experience in itself. Yeah,

Jason Hand: yeah, I like to steal Nathen Harvey?s phrase often where he's, he likes to say, We put down the pager and we picked up the microphone. Yeah, that's but that's exactly what it is. I am not the practitioner who's on call anymore, but I understand the pain of that, and now I can sort of be more of a mentor role in a coach and say let me let me help you by showing you were telling you and explaining to you the things that I've learned and that didn't go so well but that that's not something you have to do.

**Jay Gordon:** Yeah, we people that kind of made that move. It feels like it's it's you're giving up, Oh, some portion of either route or you're giving up the pager or something, but your picking up, I think a very, very valuable new weapon of choice, if you will in the microphone, cause the microphone could really help you make people better decisions on. So I guess that's what's really cool about where I guess both of us have wound up in our career, since we both made the decision at some point, put down pagers. So So let's move forward in podcast. At this point, normally likes to get into ah, a little bit more about an incident, and I don't know if we're gonna talk about a specific incident, but what I'd like you to talk to me is about, you know, your maybe a particular set of incidents that may be helped formulate some better ideas for some places that in your travels eso before we do that. You know, it's always important for me to mention that this podcast has rules. And if the enforcer of these rules I'm going to share him with you first, don't incriminate yourself, and I think you're gonna be all right on here. We're gonna go with generalities and you're not gonna incriminate others. Because the big part of this podcast is that we always remember and Dev Ops, we are blameless in our retrospectives. And the third rule is help us learn because this podcast is a retrospective. Um, so why don't you make me not necessarily an interesting incident from a nightmare? But can you give me kind of maybe a little bit of background on some of the big issues you saw that we're creating nightmares and organizations that didn't have good on call policies?

Jason Hand: Uh, so the thing I see consistently is that a lot of times Well, first of all, a lot of companies don't They haven't put enough thought or any thought in many cases into what are we gonna do when something goes wrong? They have more of a focus on. Let's do what we can to prevent things from going wrong, which if you've been doing this long enough, it seems like that were on some sort of treadmill. We can never get to this, like ideal situation of preventing problems to go wrong. And if you've studied anything about complex systems and systems thinking, you know that that's just it's just it's an impossibility. It isn't something that can be done. You can't control complex systems and prevent things from happening. So once you've sort of had that mind shift, that prevention isn't really possible, It's not reality. Then you move away from that idea and Maur into Well, if something's gonna go wrong, how do we know something's going wrong? Then what are we gonna do about it? And then the third part, which is I think what I focus on a lot two is, How do we learn from that? Jerry improved in that because if this is just going to be a routine thing, is gonna be a natural part of the work. Although it seems unplanned, what can we do to make this feel like it is more part of our work and that should be planned and that we should be thinking about when something goes wrong. First of all, what? Monitoring on observe ability. Tools are telling us that something's not quite right. And this gets into a whole conversation about setting service level indicators, objectives and error budgets and the whole s every conversation. Yeah, Then there's also the part about okay. Monitoring has alerted us that something's going not quite right or we're getting dangerously close to an imminent failure. What are we going to do about it? Who's going to be the first people that need to be made aware of these issues, what information and context they need. Thio actually do something about it. What access in the systems do they need? Who is the right person way? Talk a lot about putting Dev's on call? And there's definitely a lot of people out there who pushed back on that because they don't feel they have the experience or the access into the systems to actually do anything to help with that. And that's again the whole nother conversation about how he should be putting our developers on call and then once you've really started to figure out what is the right response and I like to remind people that we are after a response. Not we don't want to react to problems like reactionary. Some kind of has this insinuation that we weren't wanna wear something was gonna happen. We've already covered way have to be aware that something's gonna happen. So how do we improve just that response to take what might normally be a three hour outage and make it a 15 minute routine? You know, response like this. We've seen this before. We know exactly what to do in this situation, and we can mitigate it very quickly.

**Jay Gordon:** So do you think that the big problem with companies that maybe you've talked to in the past it has been walking into their room and seeing zero plan of action? Yeah, like that's the way I kind of feel. When I worked in consulting, a company would come to me and they would say, What do we do when our site goes down? You know, I asked, What are you already doing? And when they already don't have an answer for that and they just say, I don't know, we call Ted or we call, You know, somebody and Ted goes in logs in and 15 minutes later, I get a phone call that says, Hey, give me 15 more minutes. You know, that's the way I remember a lot of these things with you. And you have to start saying, Well, look, Ted ain't gonna be there all the time. And Ted's extra 15 minutes is a question, you know, Why did Ted need that extra 15 minutes? Why wasn't it part of the the actual like answer? And so I feel like I really appreciate one of the things that you have done, which is help business to say. You need a map, you need a map. You need a weight, actually an actress so that these, like you, say the three hour turns to 15 minutes. So how did you start really providing people with a map like, how does that process begins? 

Jason Hand: I thinka big part of it is that you need to have a conversation with an Engineering org, our development team. You have to have a conversation of what? What is appropriate for us. You know, I always avoid very prescriptive like approaches of like do this. I don't believe in terms like best practices. I think it's a unique situation across everyone's own scenario. But I think having that starting a conversation of saying, First of all, what monitoring do we have in place? How do we even know something's wrong? Because if it's our customers that are telling us through customer support that our sights down or that credit card processing isn't working or lot, they can't log in. If the customer knew that before you did, that's your first problem. Sure, we've got some observe ability issues. We've got some blind spots and are monitoring. So let's start there. And if you if you're familiar with that, sorry And like Google's approached us already, especially the site reliability engineering book that O'Reilly published. They talk about the hierarchy of reliability, and it always starts with monitoring. Monitoring is how you know something's not right, and then so let's. That's why start the conversation was, What are you doing with monitoring? And then from there is we'll want something What you know something's wrong. Monitoring is time to tell you that you're out of bounds. You're dangerously close. What methods do you have to inform the right people? So it's the alerting process. Are you using old school pagers with or just SMS or using email, which, by the way, terrible idea toe people on email.

**Jay Gordon:** And if you're going to third party provider like Pager Duty?s, you're Victor Ops's et cetera, et cetera. Like what Air? There s always to make sure that you're getting everything. So have how far? Deep down the stack of you. Go on, if you will. How far can you trace it To make sure that you've met all these, you don't answer so that you can say, All right, At least we know how to get what's failed. Yeah, what do you do after? So how do you tell them? Okay, what we've got in place for you is an idea of how to better at least get information. I think that the one of the hardest parts and and you can tell me that this is actually identifying signals through noise and how you actually change the culture of monitoring because you said it yourself. Observe, ability is important. But the culture of monitoring in my previous experience was we're gonna monitor all these particular service is on X server and it's gonna tell us how much off these percentages of utilization is going on and that we're going to set some arbitrary values that we believe based on a template is our go to think like How do you tell people? But your go to thing it doesn't work. Like, How do you get that lets movie to something more prescriptive rather than reactive?

Jason Hand: Yeah, well, um, the thing I like to remind people is like We all work for a business and the business is exist to provide some sort of value. Show you to the end user. Somebody's paying for a service or they're using a service. That service is providing value to them. It accomplishes the task. Maybe it's something like just, you know, logging into your bank and moving money around. Maybe it's something that's related to health care. What, you're going to get some records over to your doctor, but there's value in the technology that we're building. The value is provided by the service is that you know are built and service's are provided by applications and applications are provided by the infrastructure, and that's sort of the relationship. The lines try to create

**Jay Gordon:** a value to ensuring that the infrastructure isn't just monitored but is properly monitor

Jason Hand: right. And I think it's a matter of kind of shifting your perspective away from traditional monitoring. We would look for things that the infrastructure level and also maybe the application level. But it's actually that is, especially in a cloud world, not that interesting. It's not that important because user experience hasn't it doesn't give us any indication of what's going on with users. In fact, you could have all of your infrastructure and all everything sort of looking, maybe from a dashboard perspective. Looking read, Everything looks terrible. The whole world's on fire. Yet your users were still able to do what they're what they're trying to do. So this is where you start talking about the noises like if you're just monitoring for the technical components and the things that may fail there, you're gonna get a lot of noise, especially in a cloud situation. And that's not always actionable because a lot of times you might see a spike in CPU usage or you might see a memory problem. But a lot of times is those things will just clear themselves up because it's it's actually not that it's something else. It's having an issue, that sort of down stream, causing other problems. So

**Jay Gordon:** application acts. This is fine. You're just having I owe increase, maybe because of a background process. That's nothing to do with serving the application.

Jason Hand: Absolutely. So this is where s sorry has become. I think so. Useful for a lot of businesses is because shifts our perspective on monitoring away from the technical components, which do not give us any kind of insight into how the value of the business is is, you know, sort of being provided. What it's what it's doing is it's just telling us that there's these little technical components that are, you know, kind of breaking and fixing and breaking, fixing or just being noisy in general. So the s sorry kind of mindset shift our perspective more towards the value that we're trying to provide, and that means that we have to find a different way of monitoring. So monitoring itself has really evolved into. If you're following some of the necessary conversations, we set what we call a service level indicators, and there's a few different types of things that we feel are good indicators of the value that we're trying to provide. And it leads us in the conversations of, like, Leighton, see and you know, just congestion on the network or saturation on. There's always sort of different things that every business has to decide what is an indication, what is a leading indication of the value we're trying to provide and let's monitor for that. And so they set service level indicators that tell us or they create service level indicators that tell us over the course of time where is the sort of value existing at any time. And then we set the objectives, which is our sort of line in the sand, and we say Okay, so if this is our service level indicators, it's giving us an indication of the value of providing or not provided What is the objective? What is the line that we say If this is crossed now it's time to take action.

**Jay Gordon:** If it takes me 15 seconds to transfer 25 bucks from a point that be point that something's wrong. If it takes five seconds, that's fine. That's even if that means that there's a CPU process. It's over utilized, and maybe the five seconds is within the bounds of when you're okay. When it really should only be one or two, I get where you're coming from. That's that. You need to find out what's an acceptable wait for what humans do, and I think that really comes down to, and this is kind of outside the scope of the podcast. But it's a great thing to bring up this. This is what product experience is all about. I think in the long term is that is someone who does, like, you know, product experience, testing They're going in with users or or, you know, selenium tests or whatever and determining what is an acceptable amount of time for a process to occur and without people that actually specialize in determining how long that's gonna take what's acceptable? What can we do to improve that? Then? I think that way don't get toe what those service level like objectives are. And so I think that there's a huge part in the product process that could really help people. At the orry level even have better experiences, you know, and that's why I believe that. And here's where The Dev Ops, part of all of it kind of puts a nice, cute little hat on top, if you will. Is the devil's part is that we're opening up communication between teams when we ensure that that story in product are talking about the long term viability and how objective levels air set. What's a failure? That's okay on the customer, love. 

Jason Hand: Yeah, absolutely. I mean, I don't think that's a really, really works if you don't have product heavily involved with that. That was a model that we set up Victor ops is we created what we call the S. Sorry, counsel, because we didn't want an s sorry team we didn't want create another silo. Sure. And we didn't want saris site reliable engineers because then all the responsibility of reliability, I kind of just naturally seems to fall on those phones. Sure. And reliability is the responsibility of everyone all the way up to the top leadership because we just we have to think of it a sweet ride. It's It's more value reliability, engineering, and I like to say we're protecting the value of the business. That's the reliability of the value that we were after. It's not. It's not the reliability of the of the servers. It's not reliability, the apse, those air those air means to provide a thing that our users want. So we have to shift all of our monitoring in that direction. And then we can reduce this whole problem with noise and pager fatigue and alert fatigue and those types of things by monitoring and alerting Onley on things when when our objectives have been breached or when our error budget has been exhausted.

**Jay Gordon:** Have you ever had a person really sit down with youto help them find what, exactly, like are the right things to breach on? Like, have you spent time? And maybe this is a great way to talk about how nightmare got resolved. Yeah. Did you ever have to spend that time finding what was out of bounds and what was no longer acceptable? Because that business didn't even understand it yet? Totally. Yeah. I mean,

Jason Hand: a big part of s sorry. And what we have sort of concluded or determined at Microsoft is that s sorry, isn't like I was just mentioning It's not doesn't have to be a thing that is solely responsible for the responsibility of a group or a person. It's something we all think about. And so what I liked to do or what I like to do and what I did at Victor Ops is is really go throughout. Everyone, all the stakeholders, anybody who has a strong stake in the value and the thing that we're trying to build, including the product team, including the CEO and SE. What worries you about your business about your company? What is it that really scares you and keeps you up at night? And let's let's discuss all that. Let's get those out on, you know, on a white board or whatever, and then let's talk about how can we monitor for that? It may seem like it's an abstract thing, but we can find ways to tie us to tie those concerns back to quantitative Sure numbers.

**Jay Gordon:** How do abstract issues become resolvable problems? How do we

Jason Hand: how we alerted that on any given day at 3 p.m. We usually have, I say, usually have maybe 10,000 people in her system. That's what was it home if the system, if suddenly we went from 10,000 users down to 50 users. Wouldn't you want to know about that? Doesn't seem like it's a leading indicator of a problem with your system. Even though all your servers and your raps and everything seems fine, something isn't quite right, and it's affecting your business.

**Jay Gordon:** Yeah, it could be something as simple as a database is down in your user database is not probably, you know, a lot of people to authenticate or a portion of it is broken. You know, who knows? It could be any different number of Or maybe you're, oh, off where people are giving a specific A P I for a certain type of O off they're down. And that's why portion of you. So I guess what really what I like to think about with modern moderate and me saying that with my awful New York accent. Modern monitoring. I like to think about it in this really big scheme of things that you should be thinking about what the person with the phone or the mouse is going to be accomplishing. And when you think about the big picture of what you're looking for out of a moderate solution, it should be. How am I gonna make sure that the person on the other side of the phone poor or the mouse gets the fastest response from this application that I'm providing reliability for? And that that's the big difference is so just looking for memory utilization, Just using, you know, CPU. It's not enough anymore, Total.

Jason Hand: And to come back to you like your original question, how do you make all this better? How do you make the on call experience Better? E Think it kind of starts with that like you really have to rethink what monitoring is and should be for whatever it is that you're trying to provide your users and maybe pit on what we actually monitor. Maybe putting agents and all of the boxes isn't telling us anything other than those agents aren't behaving like they should be. But that doesn't really tell us anything about what our users air experience.

**Jay Gordon:** Yeah, I think we focus too much on the tool and not actually what's going on. But we think, well, maybe if we use sense you instead of Prometheus to get us this information, you know what I mean? Like, there's all these ideas. Let's try another tool. Maybe that other tool. Well, maybe the problem isn't the tool. Maybe you're Maggio sis. Fine. Maybe it's just you're not necessarily sitting threshold properly, and that's why you're machine is causing you an issue that moving to another platform, another tool isn't gonna fix it. It's It's taking a holistic view,  will. And I think that's where we're at

Jason Hand: absolutely that's I almost can't get through an interview without using the word holistic because we get so focused on DWI, have our special specializations, and we know what we know and what we're good at. We've just put all of our energy into that. And so we have these. We'll work with blinders on, and I think it's becoming more and more important that we take those blinders off way. We moved back to a 10,000 foot or higher view and see what is. I would come back to the term value. What is the thing? Why do we show up in write code? Why do we show up and create in for sure? What are we doing These things? It's not necessarily just to create APS.

**Jay Gordon:** People use them. It's and it's just all problems, for sure,

Jason Hand: sure. So what is the problem that they're trying to solve? And are we delivering? That's what monitoring is trying or should be trying to solve for, you know, sort of more current scenario makes a lot of sense.

**Jay Gordon:** So let's let's start wrapping it up toward the end. We've been chatting for a while, and I want to make sure that I make the most and best used at this time. So if someone is listening to you and I talk about things today and they're on a call, what's a piece

Jason Hand: of advice you give that person? I think the main thing is to really think about being on call. It's just a natural part of the job, and it's it's not something that should be dumped onto the lapse of specific people. I think it's something everybody has to take ownership in. And if we if we sort of take that mentality that this is something everybody is responsible for, there are better ways to monitor that's not so noisy and those types of things, um, it really sort of lessons, that anxiety about being on call and then you hit it makes it makes it just normal work. It's not so unplanned. It's not so reactionary. We we, in fact, the talk I gave a Q Khan yesterday. I tried to Communicate that incidents were really just a feedback mechanism of the What's going on with your system Should not bad instance, we're gonna happen. There's gonna be problems. There's gonna be blips there's gonna be made, even outages,

**Jay Gordon:** having a reason for them and knowing how to prevent them. That's that's the big thing, Not they shouldn't be seen

Jason Hand: as North you may have. No, you may have a noisy system, but that just means maybe you gotta re assess what you're monitoring

**Jay Gordon:** incidents. It's a learning experience. They're learning experience, their natural

Jason Hand: part of a complex system. So instead of trying to like, reduced, you know you want to mitigate, you know them and you want to, like, take things that are repeating problems and try to produce that or get rid of him. But to have this idea that we can do away with them or prevent them is just nonsense. And so it really comes down to making those incidents, and those problems be reassigned as just a normal part of our system. And then that means we can switch from reactionary to response. And then also the most important thing, which we really haven't had a chance to talk too much about, is when when the fire's put out, you don't just roll up the hoses and go back to work. No, you chat about it, you gotta have a deep discussion. And it's not just about what broke. I'm one of the people out there who will tell you root causes is crap. It does root cause analysis does not do anything for you, especially if you're following that very prescriptive approach of asking sort of the five wives type of mentality. I think it's better than doing nothing, but you're leaving so much information, just un explored about what really is going on with your systems. And I should say what's really going on within your socio technical systems because it's none of this happens without people, and we really have to understand, you know, when J was paged in middle of night and it took him 10 minutes to respond, I would like to know I would like to discuss more about that. It's not a problem like you mentioned earlier. We're not. This is blameless. But, you know, you were the primary. The first responder, 10 minutes to respond is a little Seems a little lengthy.

**Jay Gordon:** Yeah, what went into the 10 minutes? Exactly.

Jason Hand: And it turns out you've been on call for two weeks, you know? And also during that two weeks, there's been a lot of noise, and you're in a lot of times you would log in to your system and find out the thing is already resolved itself.

**Jay Gordon:** Or maybe maybe the 10 minutes is because at the bar. And you know what? I didn't take this seriously because I've been on call for two weeks and every other alert like it hasn't been something to take to seriously examine. And that's one of the things that you be honest with you if you're on call, recognize when you're being over paged and start to talk about it with people and and I think that that's really to sum up what you're you kind of said, if you're noticing, people are failing and time is showing, you look into why it's happening, and it may, because of your own management practices and not giving people enough rope to be a ble thio to say, You know what? I'm gonna go hang out this weekend, and maybe I'm not gonna be on call because I had a double, you know, really bad incident last week. Totally. 

Jason Hand: And then on the other side of that, too, Like there's so much that can be learned from something that we don't even identify as you know what, There's things out there that could have gone really bad, but someone did something to prevent us from from going bad or from getting much worse. You might call that a near miss, but because we didn't label it is an incident. It wasn't something that I was, actually, you know, an outage of, like, a beyond a certain severity level,

**Jay Gordon:** Just hitting the inner bounds of failure if you or just the the near bounds of failure before something really bad happened. Yeah,

Jason Hand: but if you stop and think about it, isn't it important to understand what a J. Gordon do to prevent that from half from going really south? And why wouldn't we do a retro? Why wouldn't we have a discussion?

**Jay Gordon:** Why would it even have gotten to that point like Why didn't we have scaling systems in place to be able to prevent that or whatever it is? And then on

Jason Hand: top of that, once you realized you jumped in and you started diagnosing what's going on? No. Two people are going to do the same thing with the same problem. So, like, maybe you would have approach, but I would have a different approach. And so we've got some dissent in our actions on guy. Think that that's another thing, too. There's not always a right and wrong way to to solve problems, but there's a lot of information that should be disseminated across your entire team. He about the next time we see something like this happened, something similar has got all the same symptoms. What do we as a team feel like is the appropriate response is sure here's here's what I would recommend. Let's go and check. Take a look at our logs first. Maybe we won't hop into Prometheus or in the azure monitor. Maybe log analytics like it doesn't it doesn't really matter what the tech is. Let's figure out what is the appropriate response for something like this. And, you know, you've been doing it longer than me and maybe that situation and so I can learn something. Sure, absolutely. That's what comes out of a good retro. And it doesn't often come out of nowhere.

**Jay Gordon:** Cool. So we'll wrap it up, and before we wrap it up, I wantto just let Jason, uh, get a minute and let people know about kind of what he's been up to lately because he's been a busy dude. One of the coolest things that I know you've been doing is riding around in a van. Talk about that van. Yeah. This is my

Jason Hand: about a pro master camper van late last year. And my plan was to kind of use it as a mobile office, you know, traveling all the time anyway. But when I'm home, it's kind of nice toe sit still a little bit. But I also like to get out and enjoy the Colorado lifestyle. And so, yeah, I went on this What? What I called the Dev Ops road trip recently where I had three different events all in the Western U. S. Is the first domestic events I've had all year, and So I went and I spoke a devastates Boise. Then I went up to monitor Rama Portland's you. And then I drove down sort of Oregon and California coast on the Pacific Coast Highway and ended up in San Fran Cisco for a startup event that Microsoft was putting on, where I spoke about the incident management and then and then made my way back to Denver. So I did this kind of loop for a little over 4000 miles,

**Jay Gordon:** but he flew here. Everybody wait here to New York these days.

Jason Hand: I'll drive Van out to the East Coast. But yeah, it was a super, super fun thing. I was really need to be able to still get to all of my conferences that I need to, but I wouldn't use my carbon footprint. This is something we've talked about his advocates, you know, we get shipped around all over the place and, you know, kind of wears on some of us, some for me. Anyway, I get a little bit conscientious of of you know, what I'm doing to the planet by, you know, by going all over the place and so I was able to you know, really kind of do something positive about that. Rather than flying to Boise and home in Portland home just going home, I was gonna drive and seem more of the country that I hadn't seen before. So and then along the way, you know, try to share what I'm doing and what's going on with people, a different events and the things that I'm learning and just tryto, you know, amplify more of this dev ops mentality and monitored modern, both monitoring an incident management, which turned out to be a fun little experiment. And yeah, is pretty awesome,

**Jay Gordon:** man. Well, you know what? Thank you very much for doing this today. I really appreciate it. And people confined you on Twitter at Jason hand, right? Huh? 


Jason Hand: That's correct.

**Jay Gordon:** And so we'll be right back with the wrap up this week's episode of the own call Nightmares Podcast. Thank you very much, dude. Because we're here in person. I'm gonna thank you with a good old fashioned loud handshake. 


Jason Hand: All right, 


**Jay Gordon:** We'll be right back. 


**Jay Gordon:** Well, that's it for this week of On-Call Nightmares. If you'd like to be part of the podcast, please reach out. It's really easy You can email me at OnCallNightmares@gmail.com or can also reach out very easily on Twitter. It's at @oncallnightmare. Or, if you'd like to, you can reach out to me personally, yet @jaydestro. So thanks again for listening. It's been another great episode. I appreciate your time and we'll see you next time as I continue to talk to people who have spent time on call in tech, Thanks.


